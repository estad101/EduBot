# Background Task Processing with Celery & Redis

## ğŸš€ What's New

Your application now has **asynchronous background task processing** via Celery and Redis.

This enables:
- âœ… Instant API responses (tasks run in background)
- âœ… Bulk operations without timeouts
- âœ… Scheduled tasks (cleanup, reminders)
- âœ… Better user experience
- âœ… Scalable task distribution

## ğŸ“Š Performance Comparison

### Before (Synchronous Blocking):
```python
@app.post("/send-bulk-messages")
def send_messages(phone_numbers: list, message: str):
    # Blocks request while sending ALL messages
    for phone in phone_numbers:
        send_message(phone, message)  # 1 second each = slow!
    
    return {"sent": len(phone_numbers)}
    # Response time: 100 phones = 100 seconds! âŒ
```

### After (Asynchronous Background Task):
```python
@app.post("/send-bulk-messages")
async def send_messages(phone_numbers: list, message: str):
    # Queue task and return immediately
    task = send_bulk_messages.delay(
        phone_numbers=phone_numbers,
        message=message
    )
    
    return {
        "task_id": task.id,
        "status": "queued",
        "message": "Messages will be sent in background"
    }
    # Response time: 100 phones = 50ms! âœ…
```

**Result**: 2000x faster response time

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App   â”‚  User makes request
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   Celery.delay()  Queue task immediately
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Broker   â”‚  Task queue (reliable)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Celery Worker Processes  â”‚  Process tasks in parallel
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Task Result
(stored in Redis or DB)
```

## ğŸ¯ Usage Examples

### 1. Send Bulk Messages

```python
from tasks.celery_tasks import send_bulk_messages

@app.post("/api/send-bulk-messages")
async def send_messages(request_data: dict):
    phone_numbers = request_data['phone_numbers']
    message = request_data['message']
    
    # Queue the task - returns immediately
    task = send_bulk_messages.delay(
        phone_numbers=phone_numbers,
        message=message,
        message_type='text'
    )
    
    return {
        "task_id": task.id,
        "status": "queued",
        "recipients": len(phone_numbers),
        "message": "Messages will be sent in background"
    }
```

### 2. Check Task Status

```python
from config.celery_config import celery_app

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    task = celery_app.AsyncResult(task_id)
    
    return {
        "task_id": task_id,
        "status": task.status,
        "result": task.result,
        "progress": task.info if task.state == 'PROGRESS' else None
    }
```

### 3. Send Bulk Notifications

```python
from tasks.celery_tasks import send_bulk_notifications

task = send_bulk_notifications.delay(
    user_ids=[1, 2, 3, 4, 5],
    title="New Assignment",
    message="Your homework is ready to submit"
)

# Response returns immediately with task_id
# Notifications are created in background
```

### 4. Generate Report Asynchronously

```python
from tasks.celery_tasks import generate_student_report

@app.post("/api/students/{student_id}/report")
async def create_report(student_id: int):
    task = generate_student_report.delay(student_id)
    
    return {
        "task_id": task.id,
        "status": "generating",
        "message": "Report generation started in background"
    }

# Later, user can fetch:
# GET /api/tasks/{task_id} to see results
```

### 5. Export Students to CSV

```python
from tasks.celery_tasks import export_students_csv

@app.get("/api/students/export")
async def export_students(active_only: bool = False):
    filters = {'is_active': active_only} if active_only else None
    
    task = export_students_csv.delay(filters=filters)
    
    return {
        "task_id": task.id,
        "status": "processing",
        "message": "CSV export started - you'll receive download link soon"
    }
```

## âš™ï¸ Configuration

### Celery Config (`config/celery_config.py`)

```python
# Task serialization (JSON is safe)
task_serializer='json'

# Task timeouts
task_soft_time_limit=600   # Warn after 10 minutes
task_time_limit=900        # Kill after 15 minutes

# Result expiry (results deleted after 1 hour)
result_expires=3600

# Task queues (organize by type)
task_routes={
    'tasks.messaging.*': {'queue': 'messaging'},
    'tasks.notifications.*': {'queue': 'notifications'},
    'tasks.reports.*': {'queue': 'reports'},
}

# Rate limiting (prevent overload)
task_rate_limit={
    'tasks.messaging.send_bulk_messages': '100/m',  # Max 100 msgs/minute
}
```

## ğŸš€ Running Celery Workers

### Start a Worker (Development)
```bash
celery -A config.celery_config worker --loglevel=info
```

### Start Multiple Workers (Production)
```bash
# Worker 1: Handle messaging tasks
celery -A config.celery_config worker -Q messaging -n worker1@%h

# Worker 2: Handle notifications
celery -A config.celery_config worker -Q notifications -n worker2@%h

# Worker 3: Handle reports
celery -A config.celery_config worker -Q reports -n worker3@%h
```

### Start Periodic Task Scheduler
```bash
celery -A config.celery_config beat --loglevel=info
```

## ğŸ“‹ Available Tasks

### Messaging Tasks
- `send_bulk_messages` - Send messages to multiple users
- `send_template_message` - Send template message to single user

### Notification Tasks
- `send_bulk_notifications` - Create notifications for multiple users

### Report Tasks
- `generate_student_report` - Generate detailed student report
- `export_students_csv` - Export all students to CSV file

### Scheduled Tasks
- `cleanup_old_sessions` - Runs daily, cleans up old data

## ğŸ”„ Task States

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PENDING    â”‚  Task queued, not yet started
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROGRESS   â”‚  Task is running
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SUCCESS    â”‚  Task completed successfully
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       OR

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FAILURE   â”‚  Task failed with error
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Monitoring Task Progress

```python
# For long-running tasks, show progress

@app.get("/api/tasks/{task_id}/progress")
async def get_progress(task_id: str):
    task = celery_app.AsyncResult(task_id)
    
    if task.state == 'PROGRESS':
        return {
            "status": "processing",
            "current": task.info['current'],
            "total": task.info['total'],
            "percentage": int((task.info['current'] / task.info['total']) * 100)
        }
    elif task.state == 'SUCCESS':
        return {
            "status": "completed",
            "result": task.result
        }
    elif task.state == 'FAILURE':
        return {
            "status": "failed",
            "error": str(task.info)
        }
    else:
        return {
            "status": task.state,
            "message": "Task is queued"
        }
```

## ğŸ”§ Creating Custom Tasks

Template for new tasks:

```python
from config.celery_config import celery_app
import logging

logger = logging.getLogger(__name__)

@celery_app.task(name='tasks.custom.my_task', bind=True)
def my_task(self, param1, param2):
    """
    Custom background task.
    
    Args:
        param1: First parameter
        param2: Second parameter
    """
    try:
        logger.info(f"Starting my_task with {param1}, {param2}")
        
        # Your async code here (same pattern as existing tasks)
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def do_work():
            # Your async logic
            return result
        
        result = loop.run_until_complete(do_work())
        loop.close()
        
        return result
        
    except Exception as e:
        logger.error(f"Error in my_task: {str(e)}")
        # Retry up to 3 times with 60 second delay
        self.retry(exc=e, countdown=60, max_retries=3)
```

## âš ï¸ Best Practices

1. **Keep tasks focused** - One task = one logical operation
2. **Use timeouts** - Prevent stuck tasks (configured: 15 min max)
3. **Make tasks idempotent** - Safe to run multiple times
4. **Return useful data** - Return results that can be displayed to users
5. **Use task routing** - Separate queues for different task types
6. **Monitor workers** - Check logs and task status regularly

## ğŸš¨ Troubleshooting

### Task Not Executing
- Check worker is running: `celery -A config.celery_config worker --loglevel=info`
- Check Redis is running: `redis-cli ping` (should return PONG)
- Check task routing in config

### Task Timeout
- Increase `task_time_limit` in celery config
- Break task into smaller subtasks
- Optimize the task code

### Lost Tasks
- Make sure Redis is persistent (RDB or AOF)
- Use `acks_late=True` in task config for reliability
- Monitor Redis memory usage

## ğŸ“ˆ Performance Impact

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Bulk send 100 messages | 100 seconds | 50ms | **2000x faster** |
| API response time | Blocked | Instant | **Dramatic** |
| Concurrent operations | Limited | Unlimited | **Scalable** |
| Server resources | All used | Distributed | **Optimized** |

## ğŸ¯ Next Steps

1. **Start worker process**:
   ```bash
   celery -A config.celery_config worker --loglevel=info
   ```

2. **Update high-traffic endpoints** to use tasks:
   - Bulk messaging
   - Notifications
   - Report generation
   - Data exports

3. **Monitor task execution**:
   - Check worker logs
   - Monitor Redis usage
   - Track task success/failure rates

4. **Add more tasks** as needed for other long-running operations

## ğŸ”— Resources

- [Celery Documentation](https://docs.celeryproject.org/)
- [Redis Quick Start](https://redis.io/topics/quickstart)
- [Celery Best Practices](https://docs.celeryproject.org/en/stable/userguide/tasks.html#tips-and-best-practices)
